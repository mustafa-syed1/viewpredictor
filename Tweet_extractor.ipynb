{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tools as t\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import preprocessor as p\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import json\n",
    "import StringIO\n",
    "from os import listdir\n",
    "from os.path import isfile, join,isdir\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "%matplotlib inline\n",
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_fileIn_array(filename):\n",
    "    news=np.array(pd.read_csv(filename))\n",
    "    return news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename='./geo.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_entities(news):\n",
    "    classifier = '/usr/share/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "    jar = '/usr/share/stanford-ner/stanford-ner.jar'\n",
    "    st = StanfordNERTagger(classifier,jar)\n",
    "    news_entities=[]\n",
    "    for i in range(0,len(news)):\n",
    "        temp_news=news[i][2]\n",
    "        if type(temp_news)==str:        \n",
    "            temp_news=temp_news.decode(\"utf8\")\n",
    "            sentence = word_tokenize(temp_news)\n",
    "            tagged=st.tag(sentence)\n",
    "            news_entities.append(np.array(tagged))\n",
    "    it_entities=list()\n",
    "    for entities in news_entities:\n",
    "        tagged=np.array(entities)\n",
    "        tagged=tagged[tagged[:,1]!='O']\n",
    "        tagged=tagged[tagged[:,1]=='PERSON'][:,0]\n",
    "        for tag in tagged:\n",
    "            it_entities.append(tag.encode(\"utf-8\"))\n",
    "    return it_entities     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news=read_fileIn_array(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "it_entities=get_entities(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(it_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp=np.array(it_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(it_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fname=\"tweets/entities.csv\"\n",
    "df=pd.DataFrame(it_entities)\n",
    "df.to_csv(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"entities.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=df['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr=np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_dict=dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "import datetime as dt\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "def load_api():\n",
    "\n",
    "    consumer_key = 'duhAXGFbpB72qajYFM2tnxJHb'\n",
    "    consumer_secret = '0i3Pilasyr0zLkSgEe2oTaF0URqHZCLF3t6uZ3gFT7IqDFZGz1'\n",
    "    access_token = '897788995668762624-rmsX4hvUmpCXDhCtKGgVB9Cs3JHnpve'\n",
    "    access_secret = 'kNH2m8KxQEYJWiban0yXNNhHEszfrTU5p5UjDXtXiLtMv'\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "    # load the twitter API via tweepy\n",
    "    return tweepy.API(auth)\n",
    "\n",
    "    \n",
    "def tweet_search(api, query, max_tweets, max_id, since_id, geocode):\n",
    "\n",
    "    searched_tweets = []\n",
    "    while len(searched_tweets) < max_tweets:\n",
    "        remaining_tweets = max_tweets - len(searched_tweets)\n",
    "        try:\n",
    "            new_tweets = api.search(q=query, count=remaining_tweets,\n",
    "                                    since_id=str(since_id),       max_id=str(max_id-1))\n",
    "#                                    geocode=geocode)\n",
    "            print('found',len(new_tweets),'tweets')\n",
    "            if not new_tweets:\n",
    "                print('no tweets found')\n",
    "                break\n",
    "            searched_tweets.extend(new_tweets)\n",
    "            max_id = new_tweets[-1].id\n",
    "        except tweepy.TweepError:\n",
    "            print('exception raised, waiting 15 minutes')\n",
    "            print('(until:', dt.datetime.now()+dt.timedelta(minutes=15), ')')\n",
    "            time.sleep(15*60)\n",
    "            break # stop the loop\n",
    "    return searched_tweets, max_id\n",
    "\n",
    "\n",
    "def get_tweet_id(api, date='', days_ago=9, query='a'):\n",
    "\n",
    "    if date:\n",
    "        # return an ID from the start of the given day\n",
    "        td = date + dt.timedelta(days=1)\n",
    "        tweet_date = '{0}-{1:0>2}-{2:0>2}'.format(td.year, td.month, td.day)\n",
    "        tweet = api.search(q=query, count=1, until=tweet_date)\n",
    "    else:\n",
    "        # return an ID from __ days ago\n",
    "        td = dt.datetime.now() - dt.timedelta(days=days_ago)\n",
    "        tweet_date = '{0}-{1:0>2}-{2:0>2}'.format(td.year, td.month, td.day)\n",
    "        # get list of up to 10 tweets\n",
    "        tweet = api.search(q=query, count=10, until=tweet_date)\n",
    "        print('search limit (start/stop):',tweet[0].created_at)\n",
    "        # return the id of the first tweet in the list\n",
    "        return tweet[0].id\n",
    "\n",
    "\n",
    "def write_tweets(tweets, filename):\n",
    "    ''' Function that appends tweets to a file. '''\n",
    "\n",
    "    with open(filename, 'a+') as f:\n",
    "        for tweet in tweets:\n",
    "            json.dump(tweet._json, f)\n",
    "            f.write('\\n')\n",
    "\n",
    "\n",
    "def main():\n",
    "    ''' This is a script that continuously searches for tweets\n",
    "        that were created over a given number of days. The search\n",
    "        dates and search phrase can be changed below. '''\n",
    "\n",
    "\n",
    "\n",
    "    ''' search variables: '''\n",
    "    search_phrases = arr\n",
    "    time_limit = 1.5                           # runtime limit in hours\n",
    "    max_tweets = 100                           # number of tweets per search (will be\n",
    "                                               # iterated over) - maximum is 100\n",
    "    min_days_old=0\n",
    "    max_days_old =  7                        # search limits e.g., from 7 to 8\n",
    "                                               # gives current weekday from last week,\n",
    "                                               # min_days_old=0 will search from right now\n",
    "    USA = '33.69458,73.06437,500km'       \n",
    "\n",
    "    # loop over search items,\n",
    "    # creating a new file for each\n",
    "    searched_dict=dict()\n",
    "    for search_phrase in search_phrases:\n",
    "        \n",
    "        if search_phrase not in searched_dict:\n",
    "            searched_dict[search_phrase]=0\n",
    "            print('Search phrase =', search_phrase)\n",
    "\n",
    "            ''' other variables '''\n",
    "            name = search_phrase.split()[0]\n",
    "            json_file_root = \"raw_tweets/\"+name + '/'  + name\n",
    "            temp_file_root=\"raw_tweets/\"+name \n",
    "         \n",
    "            if not isdir(temp_file_root):\n",
    "                os.makedirs(os.path.dirname(json_file_root))\n",
    "                \n",
    "            read_IDs = False\n",
    "\n",
    "            # open a file in which to store the tweets\n",
    "            if max_days_old - min_days_old == 1:\n",
    "                d = dt.datetime.now() - dt.timedelta(days=min_days_old)\n",
    "                day = '{0}-{1:0>2}-{2:0>2}'.format(d.year, d.month, d.day)\n",
    "            else:\n",
    "                d1 = dt.datetime.now() - dt.timedelta(days=max_days_old-1)\n",
    "                d2 = dt.datetime.now() - dt.timedelta(days=min_days_old)\n",
    "                day = '{0}-{1:0>2}-{2:0>2}_to_{3}-{4:0>2}-{5:0>2}'.format(\n",
    "                      d1.year, d1.month, d1.day, d2.year, d2.month, d2.day)\n",
    "            \n",
    "            json_file = json_file_root + '_' + day + '.json'\n",
    "            \n",
    "            if os.path.isfile(json_file):\n",
    "                print('Appending tweets to file named: ',json_file)\n",
    "                read_IDs = True\n",
    "\n",
    "            # authorize and load the twitter API\n",
    "            api = load_api()\n",
    "\n",
    "            # set the 'starting point' ID for tweet collection\n",
    "            if read_IDs:\n",
    "                # open the json file and get the latest tweet ID\n",
    "                with open(json_file, 'a+') as f:\n",
    "                    lines = f.readlines()\n",
    "                    max_id = json.loads(lines[-1])['id']\n",
    "                    print('Searching from the bottom ID in file')\n",
    "            else:\n",
    "                # get the ID of a tweet that is min_days_old\n",
    "                if min_days_old == 0:\n",
    "                    max_id = -1\n",
    "                else:\n",
    "                    max_id = get_tweet_id(api, days_ago=(min_days_old-1))\n",
    "            # set the smallest ID to search for\n",
    "            since_id = get_tweet_id(api, days_ago=(max_days_old-1))\n",
    "            print('max id (starting point) =', max_id)\n",
    "            print('since id (ending point) =', since_id)\n",
    "\n",
    "\n",
    "\n",
    "            ''' tweet gathering loop  '''\n",
    "            start = dt.datetime.now()\n",
    "            end = start + dt.timedelta(hours=time_limit)\n",
    "            count, exitcount = 0, 0\n",
    "            while dt.datetime.now() < end and count<5:\n",
    "                count += 1\n",
    "                print('count =',count)\n",
    "                # collect tweets and update max_id\n",
    "                tweets, max_id = tweet_search(api, search_phrase, max_tweets,\n",
    "                                              max_id=max_id, since_id=since_id,\n",
    "                                   geocode=USA )\n",
    "                # write tweets to file in JSON format\n",
    "                if tweets:\n",
    "                    write_tweets(tweets, json_file)\n",
    "                    exitcount = 0\n",
    "                else:\n",
    "                    exitcount +=1\n",
    "                    if exitcount == 3:\n",
    "                        if search_phrase == search_phrases[-1]:\n",
    "                            sys.exit('Maximum number of empty tweet strings reached - exiting')\n",
    "                        else:\n",
    "                            print('Maximum number of empty tweet strings reached - breaking')\n",
    "                            break\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweet_cleaner(filenames,name):\n",
    "    tweets=list()\n",
    "    for filename in filenames:\n",
    "        if isfile(filename):\n",
    "            with open(filename) as fr:\n",
    "                data = fr.read()\n",
    "                json_list=data.split('\\n')\n",
    "                for i in range(len(json_list)-1):\n",
    "                    obj=json.loads(json_list[i])\n",
    "                    text=obj[\"text\"]\n",
    "                    tweets.append(text.encode(\"utf-8\"))\n",
    "    fname=\"tweets/\"+name+\".csv\"\n",
    "    df=pd.DataFrame(tweets)\n",
    "    df.to_csv(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mypaths=arr\n",
    "for name in mypaths:\n",
    "    if name not in temp_dict:\n",
    "        temp_dict[name]=0\n",
    "    path=\"raw_tweets/\"+name\n",
    "    if isdir(path):\n",
    "        onlyfiles = [join(path, f) for f in listdir(path) if isfile(join(path, f))]\n",
    "        tweet_cleaner(onlyfiles,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(s, lowercase=False):\n",
    "    s = str(s[1])\n",
    "    txt=p.clean(s)\n",
    "    txt = re.sub('[\\n]',' ',txt)\n",
    "    txt = re.sub('[:[]-_]',' ',txt)\n",
    "    return word_tokenize(txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tweet_classifier(filename):\n",
    "    tweets=read_fileIn_array(filename)\n",
    "    for i in range(0,10):\n",
    "        print preprocess(tweets[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u':', u'This', u'man', u'Khawaja', u'Shams', u'is', u'a', u'Senior', u'Lawyer', u'who', u'had', u'a', u'run', u'in', u'with', u'the', u'Police', u'.', u'The', u'fact', u'that', u'whether', u'he', u'was', u'racing', u'or', u'not', u'is\\u2026']\n",
      "[u':', u'This', u'man', u'Khawaja', u'Shams', u'is', u'a', u'Senior', u'Lawyer', u'who', u'had', u'a', u'run', u'in', u'with', u'the', u'Police', u'.', u'The', u'fact', u'that', u'whether', u'he', u'was', u'racing', u'or', u'not', u'is\\u2026']\n",
      "[u'I', u'guess', u'all', u'they', u'can', u'really', u'do', u'is', u'take', u'it', u'into', u'open', u'fields', u'and', u'blow', u'it', u'up', u'!', u'!']\n",
      "[u':', u'This', u'man', u'Khawaja', u'Shams', u'is', u'a', u'Senior', u'Lawyer', u'who', u'had', u'a', u'run', u'in', u'with', u'the', u'Police', u'.', u'The', u'fact', u'that', u'whether', u'he', u'was', u'racing', u'or', u'not', u'is\\u2026']\n",
      "[u':', u'This', u'man', u'Khawaja', u'Shams', u'is', u'a', u'Senior', u'Lawyer', u'who', u'had', u'a', u'run', u'in', u'with', u'the', u'Police', u'.', u'The', u'fact', u'that', u'whether', u'he', u'was', u'racing', u'or', u'not', u'is\\u2026']\n",
      "[u':', u'This', u'man', u'Khawaja', u'Shams', u'is', u'a', u'Senior', u'Lawyer', u'who', u'had', u'a', u'run', u'in', u'with', u'the', u'Police', u'.', u'The', u'fact', u'that', u'whether', u'he', u'was', u'racing', u'or', u'not', u'is\\u2026']\n",
      "[u'What', u'videos', u'...', u'...']\n",
      "[u'Videos', u'dekho', u'boiz']\n",
      "[u':', u'The', u'Rockets', u'will', u'reportedly', u'attempt', u'to', u're-sign', u'Chris', u'Paul', u'and', u'sign', u'LeBron', u'James', u'this', u'offseason', u',', u'Rockets', u'owner', u'is', u'fine\\u2026']\n",
      "[u':', u'This', u'man', u'Khawaja', u'Shams', u'is', u'a', u'Senior', u'Lawyer', u'who', u'had', u'a', u'run', u'in', u'with', u'the', u'Police', u'.', u'The', u'fact', u'that', u'whether', u'he', u'was', u'racing', u'or', u'not', u'is\\u2026']\n"
     ]
    }
   ],
   "source": [
    "fname=\"tweets/Khawaja.csv\"\n",
    "tweet_classifier(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
