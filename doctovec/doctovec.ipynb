{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tools as t\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import re\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "%matplotlib inline\n",
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"news.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1773,)\n",
      "(1773,)\n"
     ]
    }
   ],
   "source": [
    "paragraph = np.array(data[\"paragraph\"] )\n",
    "headline = np.array(data[\"title\"] )\n",
    "\n",
    "print paragraph.shape\n",
    "print headline.shape\n",
    "\n",
    "index = ['paragraph']\n",
    "dataframe = pd.DataFrame()\n",
    "dataframe['paragraph']=paragraph\n",
    "dataframe.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def values():\n",
    "    return 0\n",
    "unique = defaultdict(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = headline.shape[0]\n",
    "\n",
    "for i in range (N):\n",
    "    line = str(headline[i])\n",
    "    line=re.sub('[\\n]',' ',line)\n",
    "    line=re.sub('[:]',' ',line)\n",
    "    line=re.sub('[-?\",()\\']',' ',line)\n",
    "\n",
    "    lines = line.split(\".\")\n",
    "    for sent in lines:\n",
    "        #print word\n",
    "        #word = word.lower().strip()\n",
    "        \n",
    "        words = sent.split()\n",
    "        for word in words:\n",
    "            word = word.lower().strip()\n",
    "            word = word.decode('utf-8')\n",
    "        #word = word.replace('\\u','')\n",
    "        #word = str(word)\n",
    "        #word = word.replace('\\u','')\n",
    "            unique[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range (N):\n",
    "    line = paragraph[i]\n",
    "    \n",
    "    lines = str(line).split(\".\")\n",
    "    for sent in lines:\n",
    "        #word = word.lower().strip()\n",
    "        sent=re.sub('[\\n]',' ',sent)\n",
    "        sent=re.sub('[:]',' ',sent)\n",
    "        sent=re.sub('[-?\",()\\']',' ',sent)\n",
    "\n",
    "        words = sent.split()\n",
    "        for word in words:\n",
    "            word = word.lower().strip()\n",
    "            word = word.decode('utf8')\n",
    "        #word = word.replace('\\u','')\n",
    "        #word = word.replace('\\xa0','')\n",
    "        #word = word.replace('\\xc2','')\n",
    "       \n",
    "        #word = word.replace('\\u','')\n",
    "        #word = word.decode(\"utf-8\").encode(\"ascii\", \"ignore\")\n",
    "        #word = re.sub(r'[^\\x00-\\x7f]',' ', word) \n",
    "            unique[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'\\u201dabrar', u'woods', u'hanging', u'woody', u'localized', u'disobeying', u'jamiat', u'crossbar', u'wracked', u'kaspersky', u'pigment', u'taj', u'bringing', u'\\xa0\\xa0sharif', u'mohra', u'kaushal', u'yonjan', u'wooden', u'dragflick', u'boylein', u'wednesday', u'hazard\\u2019s', u'patty\\xb4s', u'sumrah', u'insular', u'270', u'272', u'274', u'276', u'sustaining', u'consenting', u'\\u201cinitially', u'errors', u'shakib\\xb4s', u'deferred', u'centimeter', u'cooking', u'designing', u'databank', u'shocks', u'crouch', u'athar', u'\\u201cnab', u'brainwashed', u'affiliates', u'erum', u'countriespanama', u'china', u'affiliated', u'confronts', u'natured', u'rawalkot', u'kids', u'climbed', u'controversy', u'\\xa0gen', u'spotty', u'millimetres', u'golden', u'projection', u'@ranveersingh', u'speeches;', u'emerick', u'attacker\\u2019s', u'spacewalks', u'stern', u'\\xa0tareen', u'p45', u'position\\u2019', u'dna', u'catchy', u'insecurity', u'cannibal', u'music', u'therefore', u'afandi', u'\\xa0\\xa0fifteen', u'sermons', u'populations', u'vocation', u'yahoo', u'meteorologist', u'bhashan', u'fazlullah', u'circumstances', u'intake', u'[afghan]', u'locked', u'of\\xa0amitabh', u'time;', u'locker', u'radicalisation', u'\\u201cdeliberately', u'wang', u'pints', u'\\u2018person', u'unjust', u'wani', u'jazak', u'\\xa0\\xa0former']\n"
     ]
    }
   ],
   "source": [
    "words = unique.keys()\n",
    "#print len(words)\n",
    "print words[0:100]\n",
    "#print type(words[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./data/GoogleNews-vectors-negative300.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#vectorizer_word2Vec = CountVectorizer()\n",
    "#vectorizer_word2Vec.fit(X)\n",
    "word2vec = defaultdict()\n",
    "for w in words:\n",
    "    #w = unicode(w).replace(\"\\r\", \" \").replace(\"\\n\", \" \").replace(\"\\t\", '').replace(\"\\\"\", \"\") \n",
    "    #w = w.encode('utf-8')\n",
    "    \n",
    "    #w = w.replace('\\u','')\n",
    "    #word = word\n",
    "    if w in model:\n",
    "        word2vec[w] = model[w]\n",
    "with open(\"reduced_worc2vec\", \"wb\") as output_file:\n",
    "    pickle.dump(word2vec, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " \n",
    "with open(\"reduced_worc2vec\", \"r\") as input_file:\n",
    "    word2vec=pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20701\n",
      "[ -4.86328125e-01   8.98437500e-02   1.13281250e-01   2.48046875e-01\n",
      "  -3.67187500e-01   2.35351562e-01  -1.23291016e-02  -1.94335938e-01\n",
      "  -3.61328125e-01   9.47265625e-02  -1.63085938e-01  -5.66406250e-01\n",
      "  -1.62109375e-01  -1.93359375e-01  -3.63769531e-02   2.20703125e-01\n",
      "  -2.26562500e-01   1.16699219e-01   6.10351562e-02  -1.31835938e-01\n",
      "  -4.00390625e-02  -1.73828125e-01   4.43359375e-01  -1.15234375e-01\n",
      "  -1.31835938e-01   4.47265625e-01  -1.88476562e-01  -1.03027344e-01\n",
      "   4.39453125e-02  -3.26171875e-01  -1.78710938e-01   1.03027344e-01\n",
      "  -5.15625000e-01   1.62109375e-01  -5.46875000e-01   2.59765625e-01\n",
      "  -4.92187500e-01  -2.22167969e-02   3.35937500e-01  -3.97949219e-02\n",
      "   1.89453125e-01   2.55859375e-01   3.39843750e-01   1.19140625e-01\n",
      "   2.69531250e-01   1.00708008e-02   1.43554688e-01  -6.17187500e-01\n",
      "  -4.06250000e-01  -7.61718750e-02  -1.67236328e-02   1.28906250e-01\n",
      "   1.56250000e-01   9.96093750e-02  -3.61328125e-01  -1.02050781e-01\n",
      "  -2.69531250e-01   4.95605469e-02  -1.08398438e-01  -4.02343750e-01\n",
      "  -1.95312500e-01   1.76757812e-01  -1.73828125e-01  -1.89453125e-01\n",
      "  -8.15429688e-02  -6.17675781e-02   2.49023438e-01  -1.79687500e-01\n",
      "  -2.06054688e-01  -5.12695312e-02  -2.81250000e-01   3.16406250e-01\n",
      "  -1.14135742e-02  -7.87353516e-03  -1.25976562e-01  -3.57421875e-01\n",
      "   6.07910156e-02   1.54296875e-01  -2.23632812e-01  -3.14453125e-01\n",
      "  -5.54687500e-01   2.75390625e-01  -4.85839844e-02  -2.94921875e-01\n",
      "   9.61914062e-02  -4.15039062e-02  -6.54296875e-02   2.68554688e-02\n",
      "  -1.39648438e-01   1.64062500e-01  -1.48437500e-01   2.00195312e-01\n",
      "  -1.70898438e-01  -1.76757812e-01  -1.34765625e-01   2.81250000e-01\n",
      "  -1.01074219e-01  -2.67578125e-01   5.23437500e-01   1.28906250e-01\n",
      "  -1.06933594e-01   1.02539062e-01   2.79541016e-02   1.43554688e-01\n",
      "  -6.83593750e-02   1.40625000e-01  -3.45703125e-01   9.52148438e-02\n",
      "   8.64257812e-02   1.00097656e-01  -2.29492188e-02  -9.76562500e-02\n",
      "   8.30078125e-02   7.81250000e-03   1.80664062e-01   4.73632812e-02\n",
      "  -6.22558594e-02  -1.08398438e-01   3.02734375e-01  -1.28784180e-02\n",
      "  -7.91015625e-02   3.66210938e-02  -1.13769531e-01   5.93261719e-02\n",
      "  -1.27929688e-01   4.05273438e-02  -1.33789062e-01   2.30468750e-01\n",
      "  -6.54296875e-02  -1.78710938e-01  -1.18164062e-01   2.55859375e-01\n",
      "  -6.00585938e-02   1.41906738e-03   1.39617920e-03  -1.19140625e-01\n",
      "  -2.55859375e-01  -2.63671875e-01   2.31445312e-01   2.71484375e-01\n",
      "   2.06054688e-01  -2.45117188e-01   4.46777344e-02  -1.19628906e-01\n",
      "  -4.60937500e-01  -6.44683838e-04   6.98852539e-03  -5.32226562e-02\n",
      "   1.79687500e-01  -2.43164062e-01   4.19921875e-01  -4.33593750e-01\n",
      "  -5.41992188e-02  -3.82812500e-01  -3.16406250e-01  -8.08593750e-01\n",
      "  -1.96838379e-03   1.11816406e-01   8.54492188e-02   1.68945312e-01\n",
      "  -2.02148438e-01  -4.24804688e-02   3.14941406e-02  -1.79687500e-01\n",
      "  -7.93457031e-03  -6.79687500e-01   3.49609375e-01   1.29882812e-01\n",
      "   6.93359375e-02   3.75000000e-01  -3.82812500e-01   5.71289062e-02\n",
      "  -7.95898438e-02  -2.50000000e-01   5.10253906e-02   5.34667969e-02\n",
      "   4.35546875e-01  -4.84375000e-01   5.15136719e-02  -5.66406250e-01\n",
      "   1.92382812e-01  -5.51757812e-02  -7.47070312e-02  -2.51953125e-01\n",
      "   3.07617188e-02   3.43750000e-01   1.21093750e-01   1.86523438e-01\n",
      "   3.10546875e-01   1.52343750e-01   5.29785156e-02   2.73437500e-01\n",
      "  -2.81250000e-01   2.74658203e-03   4.14062500e-01  -3.19824219e-02\n",
      "  -1.38671875e-01   2.57568359e-02  -6.93359375e-02  -2.61718750e-01\n",
      "  -3.45703125e-01  -1.13281250e-01  -2.59765625e-01  -3.61328125e-01\n",
      "  -3.14941406e-02  -1.16210938e-01  -2.89306641e-02  -2.89062500e-01\n",
      "  -1.80664062e-01   2.02148438e-01   2.50244141e-02   7.27539062e-02\n",
      "  -1.77734375e-01  -3.46374512e-03  -1.80664062e-01  -3.36914062e-02\n",
      "  -2.77099609e-02   2.43164062e-01  -3.30078125e-01  -1.58203125e-01\n",
      "  -1.45263672e-02  -1.53320312e-01   3.45703125e-01   1.28906250e-01\n",
      "  -2.19726562e-01   7.32421875e-02   1.82617188e-01   8.25195312e-02\n",
      "   1.32812500e-01  -1.03027344e-01  -8.20312500e-02   7.91015625e-02\n",
      "   2.47070312e-01  -1.71875000e-01   1.72851562e-01   3.14941406e-02\n",
      "   1.67968750e-01   6.34765625e-02   2.25585938e-01  -3.12500000e-02\n",
      "   1.87500000e-01   3.53515625e-01   8.15429688e-02  -7.20214844e-03\n",
      "  -2.32421875e-01   1.21582031e-01  -6.93359375e-02   3.97949219e-02\n",
      "   4.93164062e-02  -1.27929688e-01   1.66992188e-01   7.32421875e-02\n",
      "   4.93164062e-02   1.18652344e-01   1.59179688e-01  -1.21582031e-01\n",
      "   1.29882812e-01  -1.38671875e-01  -1.25732422e-02  -1.98974609e-02\n",
      "   6.95800781e-03   3.88183594e-02   3.26171875e-01  -1.13769531e-01\n",
      "  -1.67968750e-01   2.45361328e-02  -3.94531250e-01  -2.50000000e-01\n",
      "   1.28906250e-01  -1.16210938e-01  -2.65625000e-01  -2.61230469e-02\n",
      "   8.34960938e-02   2.72216797e-02   2.60925293e-03  -1.36718750e-01\n",
      "   4.24804688e-02  -1.07910156e-01  -7.12890625e-02  -1.46484375e-01\n",
      "   2.67333984e-02   1.69677734e-02  -8.54492188e-02   1.40625000e-01\n",
      "  -2.15820312e-01  -2.48046875e-01  -6.54296875e-02   1.96289062e-01\n",
      "   1.05468750e-01  -4.68750000e-02  -3.18359375e-01   2.98828125e-01\n",
      "  -4.88281250e-02  -8.69140625e-02  -2.30468750e-01  -7.47070312e-02\n",
      "  -9.52148438e-02  -4.24804688e-02  -1.65039062e-01   2.39257812e-01]\n"
     ]
    }
   ],
   "source": [
    "print len(word2vec.keys())\n",
    "print word2vec[\"pakistan\"]\n",
    "#embeddings =np.zeros((paragraph.shape[0],300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range (0,paragraph.shape[0]):\n",
    "    wembedding = np.zeros((300,))\n",
    "    comment =  paragraph[i]\n",
    "    words   =  comment.split()\n",
    "    length=0\n",
    "    for word in words:\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            wembedding+=word2vec[word.lower().strip()]\n",
    "            length+=1\n",
    "        except:\n",
    "            continue\n",
    "    wembedding/=length\n",
    "    embeddings[i]=wembedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_test = np.array(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_embeddings =np.zeros((text_test.shape[0],300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "num_steps = 500\n",
    "batch_size = 128\n",
    "display_step = 100\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "num_input = 300 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 2 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18750, 300)\n",
      "(6250, 300)\n"
     ]
    }
   ],
   "source": [
    "print embeddings.shape\n",
    "print test_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "def neural_net(x):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1=tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    layer_2=tf.nn.relu(layer_2);\n",
    "    \n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct model\n",
    "logits = neural_net(X)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "# reg=tf.reduce_sum(tf.square(weights))\n",
    "# cost=tf.add(loss_op,reg)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(logits,axis=1), tf.argmax(Y,axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_test  = np.array(y_test)\n",
    "#print \n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "encoder = MultiLabelBinarizer()\n",
    "\n",
    "one_train = encoder.fit_transform(y_train[:,np.newaxis])\n",
    "one_test  = encoder.fit_transform(y_test[:,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 37.1937, Training Accuracy= 0.555\n",
      "Step 100, Minibatch Loss= 4.0682, Training Accuracy= 0.727\n",
      "Step 200, Minibatch Loss= 4.2896, Training Accuracy= 0.703\n",
      "Step 300, Minibatch Loss= 1.6118, Training Accuracy= 0.766\n",
      "Step 400, Minibatch Loss= 0.6556, Training Accuracy= 0.836\n",
      "Step 500, Minibatch Loss= 1.0273, Training Accuracy= 0.836\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.77376002)\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    #start_size=0\n",
    "    #end_size=batch_size\n",
    "    for step in range(1, num_steps+1):\n",
    "        #batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "\n",
    "        #indices=np.arange(X_train.shape[0])\n",
    "        indices=np.random.randint(0,embeddings.shape[0],size=batch_size)\n",
    "        #np.random.shuffle(indices)\n",
    "        batch_x=embeddings[indices]\n",
    "        batch_y=one_train[indices]\n",
    "        #batch_y=batch_y.reshape(batch_y.shape[0],1)\n",
    "        #Loss=0\n",
    "        #Acc=0\n",
    "        #print batch_x.shape\n",
    "        #print batch_y.shape\n",
    "\n",
    "        #print batch_x\n",
    "        #print batch_y\n",
    "        sess.run(train_op,feed_dict={X:batch_x,Y:batch_y})\n",
    "        if step %display_step == 0 or step==1:\n",
    "            \n",
    "        \n",
    "        #for j in range (X_train.shape[0]/batch_size):\n",
    "            #batch_x=X_train[indices[j*batch_size:(j+1)*batch_size]]\n",
    "            #batch_y=y_train[indices[j*batch_size:(j+1)*batch_size]]\n",
    "            \n",
    "            #sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "            #if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x, \n",
    "                                                                 Y: batch_y})\n",
    "            #Loss+=loss\n",
    "            #Acc+=acc\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "        #print \"Loss \",Loss/(X_train.shape[0]/batch_size),\" Accuracy\", Acc/(X_train.shape[0]/batch_size)\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for MNIST test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: test_embeddings,\n",
    "                                      Y: one_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
